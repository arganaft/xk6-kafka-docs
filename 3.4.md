## **Apache Calcite In-Memory: Подробное объяснение**

### **Как работает Apache Calcite в памяти**

Apache Calcite - это фреймворк для построения систем управления базами данных. Его **in-memory** модуль позволяет выполнять SQL-запросы над данными в памяти, включая JSON.

#### **Основные компоненты:**

```java
// 1. Schema - определяет структуру данных
SchemaPlus schema = Frameworks.createRootSchema(false);

// 2. Table - представление данных
Table table = new InMemoryTable(data);

// 3. Planner - планировщик запросов
FrameworkConfig config = Frameworks.newConfigBuilder()
    .defaultSchema(schema)
    .build();
Planner planner = Frameworks.getPlanner(config);
```

### **Применение к JSON**

#### **Шаг 1: Преобразование JSON в табличное представление**

```java
public class JsonTable extends AbstractTable implements ScannableTable {
    
    private final List<Map<String, Object>> jsonData;
    
    public JsonTable(String jsonArray) {
        this.jsonData = parseJsonArray(jsonArray);
    }
    
    @Override
    public Enumerable<Object[]> scan(DataContext root) {
        return new AbstractEnumerable<Object[]>() {
            @Override
            public Enumerator<Object[]> enumerator() {
                return new Enumerator<Object[]>() {
                    private int index = -1;
                    
                    @Override
                    public Object[] current() {
                        Map<String, Object> row = jsonData.get(index);
                        // Преобразуем Map в массив значений
                        return convertRowToArray(row);
                    }
                    
                    @Override
                    public boolean moveNext() {
                        return ++index < jsonData.size();
                    }
                    
                    @Override
                    public void reset() {
                        index = -1;
                    }
                    
                    @Override
                    public void close() {
                        // Очистка ресурсов
                    }
                };
            }
        };
    }
}
```

#### **Шаг 2: Регистрация схемы и выполнение запроса**

```java
public class JsonSqlQueryEngine {
    
    public String executeQuery(String jsonData, String sql) {
        // 1. Создаем корневую схему
        SchemaPlus rootSchema = Frameworks.createRootSchema(false);
        
        // 2. Создаем таблицу из JSON
        Table jsonTable = new JsonTable(jsonData);
        
        // 3. Регистрируем таблицу в схеме
        rootSchema.add("data", jsonTable);
        
        // 4. Настраиваем планировщик
        FrameworkConfig config = Frameworks.newConfigBuilder()
            .defaultSchema(rootSchema)
            .parserConfig(SqlParser.Config.DEFAULT)
            .build();
        
        // 5. Выполняем запрос
        try (Planner planner = Frameworks.getPlanner(config)) {
            SqlNode parsed = planner.parse(sql);
            SqlNode validated = planner.validate(parsed);
            RelNode rel = planner.rel(validated).rel;
            
            // 6. Исполняем запрос
            Interpreter interpreter = new Interpreter(
                DataContexts.EMPTY, 
                rel
            );
            
            // 7. Собираем результаты
            return collectResults(interpreter);
        }
    }
}
```

### **Потоковая генерация JSON без создания объектов**

#### **Проблема:**
Создание промежуточных объектов для каждого клиента потребляет много памяти.

#### **Решение: Потоковая обработка через Jackson Streaming API**

```java
public class StreamingJsonSqlExecutor {
    
    private final ObjectMapper mapper = new ObjectMapper();
    
    public void executeAndStream(
        String jsonData, 
        String sql, 
        OutputStream outputStream
    ) throws IOException {
        
        // 1. Создаем JsonGenerator для потоковой записи
        JsonGenerator generator = mapper.createGenerator(outputStream);
        
        // 2. Начинаем массив JSON
        generator.writeStartArray();
        
        // 3. Выполняем SQL и обрабатываем результаты построчно
        try (Planner planner = createPlanner(jsonData)) {
            SqlNode parsed = planner.parse(sql);
            SqlNode validated = planner.validate(parsed);
            RelNode rel = planner.rel(validated).rel;
            
            // 4. Исполняем запрос с callback'ом на каждую строку
            executeWithCallback(rel, row -> {
                try {
                    writeRowAsJson(generator, row);
                } catch (IOException e) {
                    throw new RuntimeException(e);
                }
            });
        }
        
        // 5. Завершаем массив
        generator.writeEndArray();
        generator.flush();
    }
    
    private void writeRowAsJson(JsonGenerator generator, Object[] row) 
            throws IOException {
        
        generator.writeStartObject();
        
        // Предполагаем, что знаем структуру результата
        // Например: SELECT id, name FROM data
        generator.writeNumberField("id", (Integer) row[0]);
        generator.writeStringField("name", (String) row[1]);
        
        generator.writeEndObject();
    }
}
```

### **Интеграция с WebSocket и потоковой передачей**

```java
@Component
public class WebSocketSqlHandler extends TextWebSocketHandler {
    
    @Override
    protected void handleTextMessage(
        WebSocketSession session, 
        TextMessage message
    ) throws Exception {
        
        // 1. Парсим запрос клиента
        ClientRequest request = parseRequest(message.getPayload());
        
        // 2. Получаем поток входных данных
        InputStream dataStream = getDataStream();
        
        // 3. Создаем потоковый обработчик
        try (JsonParser parser = mapper.createParser(dataStream);
             JsonGenerator generator = mapper.createGenerator(
                 new WebSocketOutputStream(session)
             )) {
            
            // 4. Начинаем JSON массив в ответе
            generator.writeStartArray();
            
            // 5. Потоково парсим входной JSON и применяем фильтры
            while (parser.nextToken() != JsonToken.END_ARRAY) {
                Map<String, Object> row = parseJsonObject(parser);
                
                // 6. Применяем SQL-фильтр (через Calcite in-memory)
                if (matchesSqlCondition(row, request.getSql())) {
                    // 7. Потоково записываем отфильтрованную строку
                    writeFilteredRow(generator, row, request.getFields());
                }
            }
            
            // 8. Завершаем JSON массив
            generator.writeEndArray();
            generator.flush();
        }
    }
}

// Кастомный OutputStream для WebSocket
class WebSocketOutputStream extends OutputStream {
    private final WebSocketSession session;
    
    public WebSocketOutputStream(WebSocketSession session) {
        this.session = session;
    }
    
    @Override
    public void write(int b) throws IOException {
        // Отправляем данные через WebSocket
        session.sendMessage(new TextMessage(new byte[]{(byte) b}));
    }
    
    @Override
    public void write(byte[] b, int off, int len) throws IOException {
        session.sendMessage(new TextMessage(b, off, len));
    }
}
```

### **Оптимизация: Гибридный подход**

```java
public class HybridJsonSqlProcessor {
    
    // Компилируем SQL в предикат Java для быстрой фильтрации
    public Predicate<Map<String, Object>> compileSqlToPredicate(String sql) {
        // 1. Парсим SQL через Calcite
        SqlNode parsed = planner.parse(sql);
        
        // 2. Преобразуем в выражение Java
        // Например: "price > 100" -> row -> (Integer)row.get("price") > 100
        
        // 3. Генерируем байткод или используем LambdaMetafactory
        return generatePredicate(parsed);
    }
    
    public void streamFilteredJson(
        InputStream inputJson,
        Predicate<Map<String, Object>> filter,
        List<String> selectedFields,
        OutputStream output
    ) throws IOException {
        
        JsonParser parser = mapper.createParser(inputJson);
        JsonGenerator generator = mapper.createGenerator(output);
        
        generator.writeStartArray();
        
        while (parser.nextToken() == JsonToken.START_OBJECT) {
            Map<String, Object> row = readJsonObject(parser);
            
            // Быстрая фильтрация через предикат
            if (filter.test(row)) {
                // Потоковая запись только выбранных полей
                writeSelectedFields(generator, row, selectedFields);
            }
        }
        
        generator.writeEndArray();
        generator.close();
    }
}
```

### **Пример полного рабочего кода**

```java
@Configuration
public class CalciteJsonConfig {
    
    @Bean
    public Planner jsonSqlPlanner() {
        // Создаем in-memory схему для JSON
        SchemaPlus rootSchema = Frameworks.createRootSchema(false);
        
        // Динамическая таблица - данные подставляются при выполнении
        rootSchema.add("json_data", new AbstractTable() {
            @Override
            public RelDataType getRowType(RelDataTypeFactory typeFactory) {
                // Динамически определяем тип на основе JSON
                return createDynamicRowType(typeFactory);
            }
        });
        
        return Frameworks.getPlanner(Frameworks.newConfigBuilder()
            .defaultSchema(rootSchema)
            .build());
    }
}

@Service
public class StreamingJsonSqlService {
    
    @Autowired
    private Planner planner;
    
    public void executeAndStream(
        String sql,
        JsonParser inputParser,
        JsonGenerator outputGenerator
    ) throws Exception {
        
        // 1. Парсим SQL
        SqlNode parsed = planner.parse(sql);
        
        // 2. Создаем временную таблицу с потоковыми данными
        Enumerable<Object[]> dataEnumerable = createStreamingEnumerable(inputParser);
        
        // 3. Выполняем запрос с потоковой передачей
        try (Interpreter interpreter = createStreamingInterpreter(
            parsed, 
            dataEnumerable
        )) {
            
            outputGenerator.writeStartArray();
            
            interpreter.forEach(row -> {
                try {
                    // Потоково пишем каждую строку
                    outputGenerator.writeStartObject();
                    for (int i = 0; i < row.length; i++) {
                        writeField(outputGenerator, row[i]);
                    }
                    outputGenerator.writeEndObject();
                } catch (IOException e) {
                    throw new UncheckedIOException(e);
                }
            });
            
            outputGenerator.writeEndArray();
        }
    }
    
    private Enumerable<Object[]> createStreamingEnumerable(
        JsonParser parser
    ) {
        return new AbstractEnumerable<Object[]>() {
            @Override
            public Enumerator<Object[]> enumerator() {
                return new JsonStreamEnumerator(parser);
            }
        };
    }
}
```

### **Преимущества этого подхода:**

1. **Минимальное потребление памяти** - данные обрабатываются потоково
2. **Нет создания промежуточных объектов** для каждого клиента
3. **Поддержка SQL** для сложных запросов
4. **Интеграция с WebSocket** для реального времени
5. **Масштабируемость** - можно обрабатывать очень большие JSON

### **Производительность:**

```java
// Бенчмарк сравнения:
// 1. Традиционный подход (полный объект на клиента):
//    Memory: O(n * c) где n-строки, c-клиенты
//    Time: O(n * c)

// 2. Потоковый подход с Calcite:
//    Memory: O(1) - постоянная память
//    Time: O(n) - один проход по данным
```

### **Рекомендации по использованию:**

1. **Кэшируйте скомпилированные SQL-предикаты** для одинаковых запросов
2. **Используйте пулы JsonParser/JsonGenerator**
3. **Ограничивайте сложность SQL** от клиентов
4. **Добавьте timeout** на выполнение запросов
5. **Мониторинг** памяти и производительности

Этот подход позволяет обрабатывать тысячи клиентов с индивидуальными фильтрами без взрывного роста потребления памяти.
