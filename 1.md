Да, конечно! Batch-вставка существенно повысит производительность. Вот обновленная версия с поддержкой batch-операций:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k6-test-scripts
  labels:
    app: syngtp-k6-load-test-postgres-writer
data:
  test.js: |
    import { check, sleep } from 'k6';
    import { Rate, Trend, Counter } from 'k6/metrics';
    import sql from 'k6/x/sql';
    import postgresDriver from 'k6/x/sql/driver/postgres';
    
    // Custom metrics
    const insertSuccessRate = new Rate('db_insert_success');
    const insertDuration = new Trend('db_insert_duration');
    const batchInsertDuration = new Trend('db_batch_insert_duration');
    const insertsCount = new Counter('db_inserts_total');
    const batchesCount = new Counter('db_batches_total');
    const errorsCount = new Counter('db_errors_total');
    
    // Batch configuration
    const BATCH_SIZE = 100; // Количество записей в одном batch
    
    // Test configuration
    export const options = {
      scenarios: {
        postgres_load: {
          executor: 'ramping-arrival-rate',
          startRate: 100,
          timeUnit: '1s',
          preAllocatedVUs: 50,
          maxVUs: 200,
          stages: [
            { duration: '10s', target: 1000 },
            { duration: '30s', target: 12000 },
            { duration: '60s', target: 12000 },
            { duration: '60s', target: 12000 },
            { duration: '60s', target: 12000 },
            { duration: '60s', target: 12000 },
            { duration: '60s', target: 12000 },
            { duration: '60s', target: 12000 },
            { duration: '10s', target: 0 },
          ],
        },
      },
      thresholds: {
        'db_insert_success': ['rate>0.95'],
        'db_batch_insert_duration': ['p(95)<2000', 'p(99)<5000'],
        'db_errors_total': ['count<1000'],
        'iteration_duration': ['p(95)<3000'],
      },
    };
    
    // Database configuration
    const dbUser = '__DB_USERNAME__';
    const dbPass = '__DB_PASSWORD__';
    const dbHost = 'tvled-kssh00010.esrt.sber.ru';
    const dbPort = '12541';
    const dbName = 'mr';
    
    // Connection string для работы с pgBouncer
    const connectionString = `postgres://${dbUser}:${dbPass}@${dbHost}:${dbPort}/${dbName}?sslmode=disable&search_path=murex&statement_cache_mode=describe`;
    
    // Sample instruments for variety
    const instruments = [
      'USD/RUB', 'EUR/USD', 'GBP/USD', 'USD/JPY',
      'BTC/USD', 'ETH/USD', 'SBER', 'GAZP'
    ];
    
    const sides = ['BUY', 'SELL'];
    
    // Global connection pool and batch buffer (one per VU)
    let db = null;
    let batchBuffer = [];
    
    function generateTrade() {
      const now = new Date();
      const isoTimestamp = now.toISOString();
      const instrument = instruments[Math.floor(Math.random() * instruments.length)];
      const side = sides[Math.floor(Math.random() * sides.length)];
      const timestamp = Date.now();
      const randomSuffix = Math.random().toString(36).substr(2, 9);
      
      const tradeId = `TRD-K6-${timestamp}-${randomSuffix}`;
      
      return {
        id: tradeId,
        trade_data: {
          tradeId: tradeId,
          exchangeTradeId: `EX-${timestamp}`,
          timestamp: isoTimestamp,
          executionTime: isoTimestamp,
          instrument: {
            symbol: instrument,
            type: instrument.includes('/') ? 'FX_SPOT' : 'EQUITY',
            exchange: 'TEST',
            currency: 'USD'
          },
          side: side,
          orderType: 'LIMIT',
          quantity: Math.floor(Math.random() * 10000) + 1000,
          price: Math.random() * 100 + 50,
          amount: Math.random() * 1000000,
          client: {
            id: `CLI-${Math.floor(Math.random() * 100)}`,
            account: `ACC-${Math.floor(Math.random() * 10)}`,
            type: 'INSTITUTIONAL'
          },
          metadata: {
            source: 'K6_TEST',
            version: '1.0',
            sequenceNumber: timestamp,
            latencyMs: Math.random() * 10
          }
        }
      };
    }
    
    function buildBatchInsertQuery(trades) {
      if (trades.length === 0) return null;
      
      let valuesParts = [];
      let params = [];
      let paramIndex = 1;
      
      for (let i = 0; i < trades.length; i++) {
        const trade = trades[i];
        valuesParts.push(`($${paramIndex}, $${paramIndex + 1}::jsonb, NOW())`);
        params.push(trade.id);
        params.push(JSON.stringify(trade.trade_data));
        paramIndex += 2;
      }
      
      const query = `
        INSERT INTO murex.trades (id, trade_data, created_at)
        VALUES ${valuesParts.join(', ')}
      `;
      
      return { query, params };
    }
    
    function flushBatch(forcedFlush = false) {
      if (batchBuffer.length === 0) return true;
      
      if (!forcedFlush && batchBuffer.length < BATCH_SIZE) {
        return true; // Не флашим если batch не заполнен
      }
      
      try {
        const batchToInsert = [...batchBuffer];
        batchBuffer = []; // Очищаем буфер сразу
        
        const batchQuery = buildBatchInsertQuery(batchToInsert);
        if (!batchQuery) return true;
        
        const startTime = Date.now();
        
        try {
          db.exec(batchQuery.query, ...batchQuery.params);
          
          const duration = Date.now() - startTime;
          batchInsertDuration.add(duration);
          insertDuration.add(duration / batchToInsert.length); // Среднее время на запись
          
          insertsCount.add(batchToInsert.length);
          batchesCount.add(1);
          
          // Отмечаем успех для каждой записи в batch
          for (let i = 0; i < batchToInsert.length; i++) {
            insertSuccessRate.add(true);
          }
          
          check(null, {
            'batch inserted successfully': () => true,
          });
          
          return true;
          
        } catch (execError) {
          const duration = Date.now() - startTime;
          batchInsertDuration.add(duration);
          errorsCount.add(1);
          
          console.error(`[VU ${__VU}] Batch insert failed (${batchToInsert.length} records): ${execError.message}`);
          
          // Отмечаем неудачу для каждой записи в batch
          for (let i = 0; i < batchToInsert.length; i++) {
            insertSuccessRate.add(false);
          }
          
          check(null, {
            'batch inserted successfully': () => false,
          });
          
          // При ошибке соединения пытаемся переподключиться
          if (execError.message.includes('connection') || execError.message.includes('closed')) {
            console.log(`[VU ${__VU}] Attempting to reconnect...`);
            try {
              db.close();
            } catch (e) {
              // Ignore close errors
            }
            db = null;
          }
          
          return false;
        }
        
      } catch (error) {
        errorsCount.add(1);
        console.error(`[VU ${__VU}] Unexpected error in flushBatch: ${error.message}`);
        return false;
      }
    }
    
    export function setup() {
      // Verify connection at start
      const testDb = sql.open(postgresDriver, connectionString);
      try {
        testDb.exec('SELECT 1');
        console.log('Database connection verified successfully');
        console.log(`Batch size configured: ${BATCH_SIZE} records per batch`);
      } catch (error) {
        console.error('Failed to connect to database:', error.message);
        throw error;
      } finally {
        testDb.close();
      }
    }
    
    export default function () {
      // Initialize connection pool once per VU
      if (db === null) {
        try {
          db = sql.open(postgresDriver, connectionString);
          batchBuffer = []; // Сбрасываем буфер при переподключении
          console.log(`[VU ${__VU}] Connection pool initialized`);
        } catch (error) {
          console.error(`[VU ${__VU}] Failed to initialize connection pool: ${error.message}`);
          errorsCount.add(1);
          insertSuccessRate.add(false);
          return;
        }
      }
      
      try {
        const trade = generateTrade();
        
        // Добавляем запись в буфер
        batchBuffer.push(trade);
        
        // Флашим batch если он достиг нужного размера
        if (batchBuffer.length >= BATCH_SIZE) {
          flushBatch(false);
        }
        
      } catch (error) {
        errorsCount.add(1);
        insertSuccessRate.add(false);
        console.error(`[VU ${__VU}] Unexpected error: ${error.message}`);
      }
    }
    
    export function teardown() {
      // Флашим оставшиеся записи в буфере
      if (batchBuffer && batchBuffer.length > 0) {
        console.log(`[VU ${__VU}] Flushing remaining ${batchBuffer.length} records...`);
        flushBatch(true);
      }
      
      if (db !== null) {
        try {
          db.close();
          console.log('Connection pool closed');
        } catch (error) {
          console.error('Error closing connection pool:', error.message);
        }
      }
      console.log('Load test completed');
    }
```

## Ключевые особенности batch-вставки:

1. **Буферизация записей**:
   - Каждый VU накапливает записи в `batchBuffer`
   - Размер batch задается константой `BATCH_SIZE = 100`

2. **Динамическое построение запроса**:
   - Функция `buildBatchInsertQuery()` создает один большой INSERT с множеством VALUES
   - Пример: `INSERT INTO ... VALUES ($1, $2, NOW()), ($3, $4, NOW()), ...`
   - Использует параметризованные запросы для безопасности

3. **Flush стратегия**:
   - Автоматический flush когда буфер достигает `BATCH_SIZE`
   - Принудительный flush в `teardown()` для оставшихся записей

4. **Метрики**:
   - `db_batch_insert_duration` - время вставки одного batch
   - `db_batches_total` - количество отправленных batches
   - `db_inserts_total` - общее количество записей

5. **Преимущества**:
   - Снижение количества round-trips к БД в 100 раз
   - Уменьшение overhead на уровне pgBouncer
   - Более эффективное использование сетевых ресурсов

Вы можете настроить `BATCH_SIZE` в зависимости от ваших требований. Рекомендуемые значения: 50-500 записей на batch.
